{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to my ML checklist If you are here, let me tell you the reason why i've created this documentation. As we know AI/ML is the future, and to be up-to-date is kinda messy sometimes for me. So therefore my learnings (and extras) here which will help you also \ud83d\ude42 What we are covering in these documentation \u2714\u2714\u2714 What is machine learning with examples. Types of Machine Learning problems. How to start creating machine learning models. Data Collection. Data Exploration. Splitting / Cross Validation. Data Preprocessing. Create the Machine Learning Model. Tuning and evaluation metrics. Test on real world. Model Inference. So Let's get started \ud83d\ude80 Note :- There is a request if you reading this documentation, please go through examples which i'm referring you.","title":"Introduction"},{"location":"#welcome-to-my-ml-checklist","text":"If you are here, let me tell you the reason why i've created this documentation. As we know AI/ML is the future, and to be up-to-date is kinda messy sometimes for me. So therefore my learnings (and extras) here which will help you also \ud83d\ude42","title":"Welcome to my ML checklist"},{"location":"#what-we-are-covering-in-these-documentation","text":"What is machine learning with examples. Types of Machine Learning problems. How to start creating machine learning models. Data Collection. Data Exploration. Splitting / Cross Validation. Data Preprocessing. Create the Machine Learning Model. Tuning and evaluation metrics. Test on real world. Model Inference.","title":"What we are covering in these documentation \u2714\u2714\u2714"},{"location":"#so-lets-get-started","text":"Note :- There is a request if you reading this documentation, please go through examples which i'm referring you.","title":"So Let's get started \ud83d\ude80"},{"location":"dataEngineering/","text":"As we know, we don't have missing values, therefore we'll perform following steps 1\ufe0f\u20e3 Encoding the categorical features 2\ufe0f\u20e3 Scaling the numerical features # Importing essential libraries import pandas as pd from sklearn import preprocessing, impute, model_selection, compose # Reading data df = pd.read_csv('data.csv') df.drop('car_ID',axis=1,inplace=True) # Split in feature and target for future purpose features = df.drop(['CarName','price'],axis=1) target = df.price # Names of numerical and categorical features numerical_features = [col for col in features.columns if features[col].dtypes!='O'] categorical_features = [col for col in features.columns if col not in numerical_features] # Train test split x_train,x_test,y_train,y_test = model_selection.train_test_split(features,target,random_state=32) # Nominal features nominal_features = ['fueltype','aspiration','doornumber','carbody','drivewheel','enginelocation','enginetype','cylindernumber','fuelsystem'] Let's look at the number of categories in each feature no_of_categories = 0 for i in categorical_features: no_of_categories = no_of_categories + len(features[i].unique()) print(f'Number of categories in every feature are {no_of_categories} \\n') print(f'Numbe of categories we got after encoding are {no_of_categories - len(categorical_features)} \\n') print('-----------------------------------------------------------------------\\n') for i in categorical_features: print(f'Number of categories in \"{i}\" named column are :- {len(features[i].unique())} \\n') Create a pipeline for data transformation transform = compose.make_column_transformer( (preprocessing.StandardScaler(),numerical_features), (preprocessing.OneHotEncoder(drop = 'first',handle_unknown='ignore'),categorical_features), remainder='passthrough' ) There is a catch here, if we are using any tree based model (decision tree, random forest etc), there is no need to scale the columns. Create another pipeline with scaler tree_transform = compose.make_column_transformer( (preprocessing.OneHotEncoder(drop = 'first',handle_unknown='ignore'),categorical_features), remainder = 'passthrough' ) Save the pipelines import joblib as jb jb.dump(transform,'cleanpipeline1.jb') jb.dump(tree_transform,'cleanpipeline2.jb') View on Github","title":"Feature Engineering"},{"location":"datacleanC/","text":"Data Cleaning \ud83e\uddf9 Let's clean data as we explored before Steps to clean data 1\ufe0f\u20e3 Remove useless columns like Cabin,Name,PassengerId 2\ufe0f\u20e3 Splitting the dataset (or cross validation) 3\ufe0f\u20e3 Handling missing values in numerical and categorical features (replacing or removing) 4\ufe0f\u20e3 Handling outliers (replacing or removing) 5\ufe0f\u20e3 Handling categorical features (encoding) 6\ufe0f\u20e3 Feature extraction (We're skipping it, because we've already less columns) 7\ufe0f\u20e3 Pipeline generation #Importing essential libraries import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns from sklearn.experimental import enable_iterative_imputer from sklearn import impute, preprocessing, pipeline, model_selection, compose import joblib # Reading the data df = pd.read_csv('spaceTrain.csv') # removing useless columns and separating independent and independent variables features = df.drop(['PassengerId','Cabin','Name','Transported'],axis = 1) target = df.Transported # Names of numerical features numerical_features = [feat for feat in features if features[feat].dtypes !='O'] # Names of categorical features categorical_features = [feat for feat in features if feat not in numerical_features] Before doing any cleaning/preprocessing, split the dataset into training and testing data # Split your dataset into training and testing dataset x_train,x_test,y_train,y_test = model_selection.train_test_split(features,target,test_size = 0.2,stratify=target) You can check the length of training and testing data to verify ,it si splitted or not. len(x_train),len(x_test),len(y_train),len(y_test) Handle missing values in numerical features and categorical features We have to impute missing values such that each columns kernel distribution becomes gaussian i.e. bell curve plt.figure(figsize=(10,10)) j=1 for i in numerical_features: plt.subplot(3,2,j) sns.kdeplot(data = df,x=i) j+=1 Distribution is not normal/Gaussian but near to normal # Create instance of each type of imputer available in sklearn library mean_impute = impute.SimpleImputer(strategy='mean') median_impute = impute.SimpleImputer(strategy='median') iterative_impute = impute.IterativeImputer(random_state = 30) knn_impute = impute.KNNImputer(n_neighbors=3) #Transform the data with correspondent transformers with_mean = mean_impute.fit_transform(x_train[numerical_features]) with_median = median_impute.fit_transform(x_train[numerical_features]) with_iterative = iterative_impute.fit_transform(x_train[numerical_features]) with_knn = knn_impute.fit_transform(x_train[numerical_features]) # Visualize the columns after imputing with mean plt.figure(figsize=(10,10)) j=1 for i in pd.DataFrame(with_mean).columns: plt.subplot(3,2,j) sns.kdeplot(x=with_mean[i]) j+=1 We can visualize other columns distribution using above code of other imputations On observing each distribution on each imputation, we can see all distributions are same , so we can choose any imputation technique. For e.g. let's choose mean imputation # Now let's impute categorical features # Create instance of imputation mode_impute = impute.SimpleImputer(strategy='most_frequent') const_impute = impute.SimpleImputer(strategy='constant',fill_value='not_specified') # As we know HomePlanet and Destination are colums which we are imputing with constant value like 'not_specified' and CryoSleep and VIP with their mode value. for_mode_feat = ['CryoSleep','VIP'] for_constant_feat = ['HomePlanet','Destination'] with_mode = mode_impute.fit_transform(x_train[for_mode_feat]) with_const = const_impute.fit_transform(x_train[for_constant_feat]) # We can confirm that values are imputed pd.DataFrame(with_mode).isnull().sum().sum(),pd.DataFrame(with_const).isnull().sum().sum() Now let's encode the categorical columns Keeping in mind that ordinal and nominal specificness Ordinal features are those categorical features which has values comparable We have two features which are VIP and CryoSleep Nominal features are those categorical features which has values uncomparable We have two features which are HomePlanet and Destination # Create instances for ordinal and one hot encoder ordinal_encoder = preprocessing.OrdinalEncoder() onehot_encoder = preprocessing.OneHotEncoder(drop='first') # Fit the data in encoders with_ordinal = ordinal_encoder.fit_transform(with_mode) with_onehot = onehot_encoder.fit_transform(with_const) We've encoded the categorical columns successfully Now let's deal with outliers def CustomSampler_IQR (X, y): features = X.columns df = X.copy() df['Transported'] = y indices = [x for x in df.index] out_indexlist = [] for col in features: upper_indices = [] lower_indices = [] #Using nanpercentile instead of percentile because of nan values Q1 = np.nanpercentile(df[col], 25.) Q3 = np.nanpercentile(df[col], 75.) cut_off = (Q3 - Q1) * 1.5 upper, lower = Q3 + cut_off, Q1 - cut_off upper_indices = df[col][df[col] < lower].index.tolist() lower_indices = df[col][df[col] > upper].index.tolist() X.loc[upper_indices][col] = upper X.loc[lower_indices][col] = lower #outliers = df[col][(df[col] < lower) | (df[col] > upper)].values #out_indexlist.extend(outliers_index) #using set to remove duplicates #out_indexlist = list(set(out_indexlist)) #clean_data = np.setdiff1d(indices,out_indexlist) return X, y dfa = pd.DataFrame(with_mean,columns=numerical_features).reset_index(drop=True) # with_mean has no column names, therefore we've fixed it and reset the indices clean_x,clean_y = CustomSampler_IQR(dfa,y_train.reset_index(drop=True)) If you want to remove or replace outliers, this function helps greatly, above function is replacing the outliers with upper and lower value, Commented code will helps to remove the outliers. Just pass the independent features data, function will deals with outlier in each numerical columns. There is a catch is removing outliers, because if there is so much row which have outlying values, we can remove because it results to loss of data It's upto you, you can use it or not !! Now let's create a pipeline for data preprocessing # Column transformer transform columns parallely, so make you pass different columns on every transformer inside it. # Imputing Transformer imputer = compose.make_column_transformer( (impute.SimpleImputer(strategy='mean'),[3,5,6,7,8,9]), (impute.SimpleImputer(strategy='most_frequent'),[1,4]), (impute.SimpleImputer(strategy='constant',fill_value='not_specified'),[0,2]), remainder='passthrough' ) # Categorical Transformer encoding = compose.make_column_transformer( (preprocessing.OrdinalEncoder(),[6,7]), (preprocessing.OneHotEncoder(drop='first'),[8,9]), remainder='passthrough' ) # Pipeline executes the transformer (or group of) sequentially cleaning_pipeline = pipeline.make_pipeline(imputer,encoding) As we can see above, columns are passed in terms of numbered position because, on applying transformer, our data will loose its column names therefore provide columns through numbers. As we are passing whole data, columns which are transforming first placed at beginning automatically. Save the pipeline if required # Saving the pipeline joblib.dump(cleaning_pipeline,'clean.joblib') # Loading the saved pipeline clean_pipe = joblib.load('clean.joblib') # use 'clean_pipe same as we use pipeline' Note - Don't worry about losing the column names on applying transformer, sklearn will handle all this. View on Github","title":"Data Cleaning"},{"location":"datacleanC/#data-cleaning","text":"Let's clean data as we explored before","title":"Data Cleaning \ud83e\uddf9"},{"location":"datacleanC/#steps-to-clean-data","text":"1\ufe0f\u20e3 Remove useless columns like Cabin,Name,PassengerId 2\ufe0f\u20e3 Splitting the dataset (or cross validation) 3\ufe0f\u20e3 Handling missing values in numerical and categorical features (replacing or removing) 4\ufe0f\u20e3 Handling outliers (replacing or removing) 5\ufe0f\u20e3 Handling categorical features (encoding) 6\ufe0f\u20e3 Feature extraction (We're skipping it, because we've already less columns) 7\ufe0f\u20e3 Pipeline generation #Importing essential libraries import pandas as pd import numpy as np import matplotlib.pyplot as plt import seaborn as sns from sklearn.experimental import enable_iterative_imputer from sklearn import impute, preprocessing, pipeline, model_selection, compose import joblib # Reading the data df = pd.read_csv('spaceTrain.csv') # removing useless columns and separating independent and independent variables features = df.drop(['PassengerId','Cabin','Name','Transported'],axis = 1) target = df.Transported # Names of numerical features numerical_features = [feat for feat in features if features[feat].dtypes !='O'] # Names of categorical features categorical_features = [feat for feat in features if feat not in numerical_features] Before doing any cleaning/preprocessing, split the dataset into training and testing data # Split your dataset into training and testing dataset x_train,x_test,y_train,y_test = model_selection.train_test_split(features,target,test_size = 0.2,stratify=target) You can check the length of training and testing data to verify ,it si splitted or not. len(x_train),len(x_test),len(y_train),len(y_test)","title":"Steps to clean data"},{"location":"datacleanC/#handle-missing-values-in-numerical-features-and-categorical-features","text":"We have to impute missing values such that each columns kernel distribution becomes gaussian i.e. bell curve plt.figure(figsize=(10,10)) j=1 for i in numerical_features: plt.subplot(3,2,j) sns.kdeplot(data = df,x=i) j+=1 Distribution is not normal/Gaussian but near to normal # Create instance of each type of imputer available in sklearn library mean_impute = impute.SimpleImputer(strategy='mean') median_impute = impute.SimpleImputer(strategy='median') iterative_impute = impute.IterativeImputer(random_state = 30) knn_impute = impute.KNNImputer(n_neighbors=3) #Transform the data with correspondent transformers with_mean = mean_impute.fit_transform(x_train[numerical_features]) with_median = median_impute.fit_transform(x_train[numerical_features]) with_iterative = iterative_impute.fit_transform(x_train[numerical_features]) with_knn = knn_impute.fit_transform(x_train[numerical_features]) # Visualize the columns after imputing with mean plt.figure(figsize=(10,10)) j=1 for i in pd.DataFrame(with_mean).columns: plt.subplot(3,2,j) sns.kdeplot(x=with_mean[i]) j+=1 We can visualize other columns distribution using above code of other imputations","title":"Handle missing values in numerical features and categorical features"},{"location":"datacleanC/#on-observing-each-distribution-on-each-imputation-we-can-see-all-distributions-are-same-so-we-can-choose-any-imputation-technique-for-eg-lets-choose-mean-imputation","text":"# Now let's impute categorical features # Create instance of imputation mode_impute = impute.SimpleImputer(strategy='most_frequent') const_impute = impute.SimpleImputer(strategy='constant',fill_value='not_specified') # As we know HomePlanet and Destination are colums which we are imputing with constant value like 'not_specified' and CryoSleep and VIP with their mode value. for_mode_feat = ['CryoSleep','VIP'] for_constant_feat = ['HomePlanet','Destination'] with_mode = mode_impute.fit_transform(x_train[for_mode_feat]) with_const = const_impute.fit_transform(x_train[for_constant_feat]) # We can confirm that values are imputed pd.DataFrame(with_mode).isnull().sum().sum(),pd.DataFrame(with_const).isnull().sum().sum()","title":"On observing each distribution on each imputation, we can see all distributions are same , so we can choose any imputation technique. For e.g. let's choose mean imputation"},{"location":"datacleanC/#now-lets-encode-the-categorical-columns","text":"Keeping in mind that ordinal and nominal specificness Ordinal features are those categorical features which has values comparable We have two features which are VIP and CryoSleep Nominal features are those categorical features which has values uncomparable We have two features which are HomePlanet and Destination # Create instances for ordinal and one hot encoder ordinal_encoder = preprocessing.OrdinalEncoder() onehot_encoder = preprocessing.OneHotEncoder(drop='first') # Fit the data in encoders with_ordinal = ordinal_encoder.fit_transform(with_mode) with_onehot = onehot_encoder.fit_transform(with_const) We've encoded the categorical columns successfully","title":"Now let's encode the categorical columns"},{"location":"datacleanC/#now-lets-deal-with-outliers","text":"def CustomSampler_IQR (X, y): features = X.columns df = X.copy() df['Transported'] = y indices = [x for x in df.index] out_indexlist = [] for col in features: upper_indices = [] lower_indices = [] #Using nanpercentile instead of percentile because of nan values Q1 = np.nanpercentile(df[col], 25.) Q3 = np.nanpercentile(df[col], 75.) cut_off = (Q3 - Q1) * 1.5 upper, lower = Q3 + cut_off, Q1 - cut_off upper_indices = df[col][df[col] < lower].index.tolist() lower_indices = df[col][df[col] > upper].index.tolist() X.loc[upper_indices][col] = upper X.loc[lower_indices][col] = lower #outliers = df[col][(df[col] < lower) | (df[col] > upper)].values #out_indexlist.extend(outliers_index) #using set to remove duplicates #out_indexlist = list(set(out_indexlist)) #clean_data = np.setdiff1d(indices,out_indexlist) return X, y dfa = pd.DataFrame(with_mean,columns=numerical_features).reset_index(drop=True) # with_mean has no column names, therefore we've fixed it and reset the indices clean_x,clean_y = CustomSampler_IQR(dfa,y_train.reset_index(drop=True)) If you want to remove or replace outliers, this function helps greatly, above function is replacing the outliers with upper and lower value, Commented code will helps to remove the outliers. Just pass the independent features data, function will deals with outlier in each numerical columns. There is a catch is removing outliers, because if there is so much row which have outlying values, we can remove because it results to loss of data It's upto you, you can use it or not !!","title":"Now let's deal with outliers"},{"location":"datacleanC/#now-lets-create-a-pipeline-for-data-preprocessing","text":"# Column transformer transform columns parallely, so make you pass different columns on every transformer inside it. # Imputing Transformer imputer = compose.make_column_transformer( (impute.SimpleImputer(strategy='mean'),[3,5,6,7,8,9]), (impute.SimpleImputer(strategy='most_frequent'),[1,4]), (impute.SimpleImputer(strategy='constant',fill_value='not_specified'),[0,2]), remainder='passthrough' ) # Categorical Transformer encoding = compose.make_column_transformer( (preprocessing.OrdinalEncoder(),[6,7]), (preprocessing.OneHotEncoder(drop='first'),[8,9]), remainder='passthrough' ) # Pipeline executes the transformer (or group of) sequentially cleaning_pipeline = pipeline.make_pipeline(imputer,encoding) As we can see above, columns are passed in terms of numbered position because, on applying transformer, our data will loose its column names therefore provide columns through numbers. As we are passing whole data, columns which are transforming first placed at beginning automatically.","title":"Now let's create a pipeline for data preprocessing"},{"location":"datacleanC/#save-the-pipeline-if-required","text":"# Saving the pipeline joblib.dump(cleaning_pipeline,'clean.joblib') # Loading the saved pipeline clean_pipe = joblib.load('clean.joblib') # use 'clean_pipe same as we use pipeline' Note - Don't worry about losing the column names on applying transformer, sklearn will handle all this. View on Github","title":"Save the pipeline if required"},{"location":"dataexpC/","text":"Data Exploration Before starting, we are building one Classification and Regression . First let's begin with classification. About Data In this we are selecting the data from kaggle namely Spaceship Titanic which is basically made for classification problem. # Importing essential libraries import pandas as pd import seaborn as sns import matplotlib.pyplot as plt pandas - It is used for data wrangling. seaborn & matplotlib - It is used for plotting some visualisations. Let's see some description of columns our dataset has - PassengerId A unique Id for each passenger. Each Id takes the form gggg_pp where gggg indicates a group the passenger is travelling with and pp is their number within the group. People in a group are often family members, but not always. HomePlanet The planet the passenger departed from, typically their planet of permanent residence. CryoSleep Indicates whether the passenger elected to be put into suspended animation for the duration of the voyage. Passengers in cryosleep are confined to their cabins. Cabin The cabin number where the passenger is staying. Takes the form deck/num/side, where side can be either P for Port or S for Starboard. Destination The planet the passenger will be debarking to. Age The age of the passenger. VIP Whether the passenger has paid for special VIP service during the voyage. RoomService , FoodCourt , ShoppingMall , Spa *, VRDeck** Amount the passenger has billed at each of the Spaceship Titanic's many luxury amenities. Name The first and last names of the passenger. Transported Whether the passenger was transported to another dimension. This is the target, the column you are trying to predict. #Download the dataset and use it locally from folder #Importing data df = pd.read_csv('spaceTrain.csv') df = df.iloc[:,1:] # Not including columns 'PassengerId' df.head() We are not including 'PassengerId' column because it has unique value at each row or tuple i.e. so many values. Further we can also remove 'Cabin' and 'Name' column also, because they have also many values. # Displays basic information about dataset df.describe() # unique count of values, min of column, max of column etc.. It analyzes information only about numerical columns # Numerical and categorical features no_of_numerical_features = 0 no_of_categorical_features = 0 for feat in df.columns: if df[feat].dtypes!='O': no_of_numerical_features+=1 else : no_of_categorical_features+=1 print(f'There are {no_of_numerical_features} numerical features and {no_of_categorical_features} categorical features') There are 7 numerical features and 6 categorical features # Missing Values in each independent variable missing_values = pd.DataFrame({'Features':list(df.columns),'Missing Values':list(df.isnull().sum())}) Above code displays the dataframe having all columns and their corresponding number of missing values present in it. # Let's plot missing values plt.figure(figsize=(15,8)) sns.barplot(data = missing_values,x='Features',y='Missing Values') # It will plot bars having height determining the number of missing values # Split the data in independent and dependent variable features = df.drop(['Transported'],axis = 1) target = df['Transported'] # Names of numerical and categorical features numerical_features = [feat for feat in features.columns if features[feat].dtypes!='O'] categorical_features = [feat for feat in features.columns if features[feat].dtypes=='O'] Numerical Columns - [ 'Age', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck' ] Categorical Columns - [ 'HomePlanet', 'CryoSleep', 'Cabin', 'Destination', 'VIP', 'Name' ] # Distribution of values in categorical features plt.figure(figsize=(12,12)) def label_function(val): return f'{val / 100 * len(df):.0f}\\n{val:.0f}%' j=1 for i in categorical_features: plt.subplot(3,2,j) df.groupby(i).size().plot(kind='pie', autopct=label_function, textprops={'fontsize': 12}) j+=1 Above code displays the distribution of values of every columns through pie chart Correlations are also an important measure to identify the relation between each column # Correlation between each feature correlations = features.corr() correlations Columns which have less relation value(near to 0), can be removed also # Visualize the correlation using heatmap sns.heatmap(correlations) Now, we also need to detect and visualize outliers using boxplot ## For numerical features plt.figure(figsize=(12,12)) i=1 for feat in numerical_features: plt.subplot(3,2,i) sns.boxplot(df[feat]) i+=1 View on Github","title":"Data Exploration (Classification)"},{"location":"dataexpC/#data-exploration","text":"","title":"Data Exploration"},{"location":"dataexpC/#before-starting-we-are-building-one-classification-and-regression-first-lets-begin-with-classification","text":"About Data In this we are selecting the data from kaggle namely Spaceship Titanic which is basically made for classification problem. # Importing essential libraries import pandas as pd import seaborn as sns import matplotlib.pyplot as plt pandas - It is used for data wrangling. seaborn & matplotlib - It is used for plotting some visualisations.","title":"Before starting, we are building one Classification and Regression. First let's begin with classification."},{"location":"dataexpC/#lets-see-some-description-of-columns-our-dataset-has-","text":"PassengerId A unique Id for each passenger. Each Id takes the form gggg_pp where gggg indicates a group the passenger is travelling with and pp is their number within the group. People in a group are often family members, but not always. HomePlanet The planet the passenger departed from, typically their planet of permanent residence. CryoSleep Indicates whether the passenger elected to be put into suspended animation for the duration of the voyage. Passengers in cryosleep are confined to their cabins. Cabin The cabin number where the passenger is staying. Takes the form deck/num/side, where side can be either P for Port or S for Starboard. Destination The planet the passenger will be debarking to. Age The age of the passenger. VIP Whether the passenger has paid for special VIP service during the voyage. RoomService , FoodCourt , ShoppingMall , Spa *, VRDeck** Amount the passenger has billed at each of the Spaceship Titanic's many luxury amenities. Name The first and last names of the passenger. Transported Whether the passenger was transported to another dimension. This is the target, the column you are trying to predict. #Download the dataset and use it locally from folder #Importing data df = pd.read_csv('spaceTrain.csv') df = df.iloc[:,1:] # Not including columns 'PassengerId' df.head() We are not including 'PassengerId' column because it has unique value at each row or tuple i.e. so many values. Further we can also remove 'Cabin' and 'Name' column also, because they have also many values. # Displays basic information about dataset df.describe() # unique count of values, min of column, max of column etc.. It analyzes information only about numerical columns # Numerical and categorical features no_of_numerical_features = 0 no_of_categorical_features = 0 for feat in df.columns: if df[feat].dtypes!='O': no_of_numerical_features+=1 else : no_of_categorical_features+=1 print(f'There are {no_of_numerical_features} numerical features and {no_of_categorical_features} categorical features') There are 7 numerical features and 6 categorical features # Missing Values in each independent variable missing_values = pd.DataFrame({'Features':list(df.columns),'Missing Values':list(df.isnull().sum())}) Above code displays the dataframe having all columns and their corresponding number of missing values present in it. # Let's plot missing values plt.figure(figsize=(15,8)) sns.barplot(data = missing_values,x='Features',y='Missing Values') # It will plot bars having height determining the number of missing values # Split the data in independent and dependent variable features = df.drop(['Transported'],axis = 1) target = df['Transported'] # Names of numerical and categorical features numerical_features = [feat for feat in features.columns if features[feat].dtypes!='O'] categorical_features = [feat for feat in features.columns if features[feat].dtypes=='O'] Numerical Columns - [ 'Age', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck' ] Categorical Columns - [ 'HomePlanet', 'CryoSleep', 'Cabin', 'Destination', 'VIP', 'Name' ] # Distribution of values in categorical features plt.figure(figsize=(12,12)) def label_function(val): return f'{val / 100 * len(df):.0f}\\n{val:.0f}%' j=1 for i in categorical_features: plt.subplot(3,2,j) df.groupby(i).size().plot(kind='pie', autopct=label_function, textprops={'fontsize': 12}) j+=1 Above code displays the distribution of values of every columns through pie chart Correlations are also an important measure to identify the relation between each column # Correlation between each feature correlations = features.corr() correlations Columns which have less relation value(near to 0), can be removed also # Visualize the correlation using heatmap sns.heatmap(correlations)","title":"Let's see some description of columns our dataset has -"},{"location":"dataexpC/#now-we-also-need-to-detect-and-visualize-outliers-using-boxplot","text":"## For numerical features plt.figure(figsize=(12,12)) i=1 for feat in numerical_features: plt.subplot(3,2,i) sns.boxplot(df[feat]) i+=1 View on Github","title":"Now, we also need to detect and visualize outliers using boxplot"},{"location":"dataexpR/","text":"Data Exploration Let's go ahead with Regression problems About Data \ud83d\udcbd In this we are selecting the data from kaggle namely Car Prices Datset which is basically made for regression problem. Let begin \ud83d\udc68\u200d\ud83d\udcbb # Importing essential libraries import pandas as pd import seaborn as sns import matplotlib.pyplot as plt pandas - It is used for data wrangling. seaborn & matplotlib - It is used for plotting some visualisations. Take a brief look at the dataset and learn the description on each column. # Reading data df = pd.read_csv('data.csv') df.drop('car_ID',axis=1,inplace=True) # Split in feature and target for future purpose features = df.drop('price',axis=1) target = df.price # Names of numerical and categorical features numerical_features = [col for col in features.columns if features[col].dtypes!='O'] categorical_features = [col for col in features.columns if col not in numerical_features] Names of numerical and categorical features print(f'Numerical Features \\n {numerical_features}, \\nCategorical Features \\n {categorical_features}') Numerical Features - ['symboling', 'wheelbase', 'carlength', 'carwidth', 'carheight', 'curbweight', 'enginesize', 'boreratio', 'stroke', 'compressionratio', 'horsepower', 'peakrpm', 'citympg', 'highwaympg'] Categorical Features - ['CarName', 'fueltype', 'aspiration', 'doornumber', 'carbody', 'drivewheel', 'enginelocation', 'enginetype', 'cylindernumber', 'fuelsystem', 'price'] Getting the basic information about dataset like count, min, max, 25 - 50 - 75 quantile. df.describe() Let's look at the number of null values in each columns # Great !! we have no null values df.isnull().sum() Let's plot kernel distribution for numerical features plt.figure(figsize=(20,40)) j=1 for i in numerical_features: plt.subplot(7,2,j) sns.kdeplot(df[i]) j+=1 # Not every feature's distribution is normal, so we'll handle it using feature scaling Let's look at the distribution of categories in categorical features plt.figure(figsize=(38,38)) def label_function(val): #return f'{val / 100 * len(df):.0f}\\n{val:.0f}%' return f'{val:.0f}%' j=1 for i in categorical_features: plt.subplot(11,1,j) df.groupby(i).size().plot(kind='pie', autopct=label_function, textprops={'fontsize': 12}) j+=1 From above code, we can see that CarName has lots of categories so we can ignore it (useless column) Bar plot distribution of categorical features \ud83d\udcca plt.figure(figsize=(38,38)) j=1 for i in categorical_features: if(i == 'price'): break plt.subplot(5,2,j) sns.barplot(data = df,x=i,y='price') j+=1 Feature correlation is also important aspect in featue engineering, therefore we can visualize it heatmap of correlation \ud83d\uddfa\ufe0f sns.heatmap(df.corr()) Note - Less descriptive because all the steps are kinda similar View on Github","title":"Data Exploration (Regression)"},{"location":"dataexpR/#data-exploration","text":"","title":"Data Exploration"},{"location":"dataexpR/#lets-go-ahead-with-regression-problems","text":"About Data \ud83d\udcbd In this we are selecting the data from kaggle namely Car Prices Datset which is basically made for regression problem. Let begin \ud83d\udc68\u200d\ud83d\udcbb # Importing essential libraries import pandas as pd import seaborn as sns import matplotlib.pyplot as plt pandas - It is used for data wrangling. seaborn & matplotlib - It is used for plotting some visualisations. Take a brief look at the dataset and learn the description on each column. # Reading data df = pd.read_csv('data.csv') df.drop('car_ID',axis=1,inplace=True) # Split in feature and target for future purpose features = df.drop('price',axis=1) target = df.price # Names of numerical and categorical features numerical_features = [col for col in features.columns if features[col].dtypes!='O'] categorical_features = [col for col in features.columns if col not in numerical_features] Names of numerical and categorical features print(f'Numerical Features \\n {numerical_features}, \\nCategorical Features \\n {categorical_features}') Numerical Features - ['symboling', 'wheelbase', 'carlength', 'carwidth', 'carheight', 'curbweight', 'enginesize', 'boreratio', 'stroke', 'compressionratio', 'horsepower', 'peakrpm', 'citympg', 'highwaympg'] Categorical Features - ['CarName', 'fueltype', 'aspiration', 'doornumber', 'carbody', 'drivewheel', 'enginelocation', 'enginetype', 'cylindernumber', 'fuelsystem', 'price'] Getting the basic information about dataset like count, min, max, 25 - 50 - 75 quantile. df.describe() Let's look at the number of null values in each columns # Great !! we have no null values df.isnull().sum() Let's plot kernel distribution for numerical features plt.figure(figsize=(20,40)) j=1 for i in numerical_features: plt.subplot(7,2,j) sns.kdeplot(df[i]) j+=1 # Not every feature's distribution is normal, so we'll handle it using feature scaling Let's look at the distribution of categories in categorical features plt.figure(figsize=(38,38)) def label_function(val): #return f'{val / 100 * len(df):.0f}\\n{val:.0f}%' return f'{val:.0f}%' j=1 for i in categorical_features: plt.subplot(11,1,j) df.groupby(i).size().plot(kind='pie', autopct=label_function, textprops={'fontsize': 12}) j+=1 From above code, we can see that CarName has lots of categories so we can ignore it (useless column) Bar plot distribution of categorical features \ud83d\udcca plt.figure(figsize=(38,38)) j=1 for i in categorical_features: if(i == 'price'): break plt.subplot(5,2,j) sns.barplot(data = df,x=i,y='price') j+=1 Feature correlation is also important aspect in featue engineering, therefore we can visualize it heatmap of correlation \ud83d\uddfa\ufe0f sns.heatmap(df.corr()) Note - Less descriptive because all the steps are kinda similar View on Github","title":"Let's go ahead with Regression problems"},{"location":"modelSelectC/","text":"Now let's choose the best machine learning model from wide variety of range \ud83d\udd05 # Import essential libraries import pandas as pd from sklearn import linear_model, tree, ensemble, svm, neighbors, naive_bayes, neural_network, model_selection,impute,preprocessing,pipeline,compose from sklearn import metrics from xgboost import XGBClassifier import joblib as jb # Loading the data preprocessing pipeline cleanIt = jb.load('clean.joblib') # Read the data df = pd.read_csv('spaceTrain.csv') # removing useless columns features = df.drop(['PassengerId','Cabin','Name','Transported'],axis = 1) target = df.Transported # Names of numerical and categorical features numerical_features = [feat for feat in features if features[feat].dtypes !='O'] categorical_features = [feat for feat in features if feat not in numerical_features] # Split your dataset into training and testing dataset x_train,x_test,y_train,y_test = model_selection.train_test_split(features,target,test_size = 0.2,stratify=target) Now we're creating a model which uses different models and find their accuracy measures in form of dataframe \u2696\ufe0f\u2696\ufe0f def compare_models(x_train,x_test,y_train,y_test): score_table = { 'Classifiers': ['Logistic Regression','Stochastic Gradient','Decision Tree','Random Forest','ADABoost','XGBoost','Support Vector','Naive Bayes','MultiLayer Perceptron'], 'Accuracy Score':[], 'ROC_AUC':[], 'F1': [] } models = {'Logreg':linear_model.LogisticRegression(),'sgd':linear_model.SGDClassifier(),'dt':tree.DecisionTreeClassifier(),'rf':ensemble.RandomForestClassifier(),'ada':ensemble.AdaBoostClassifier(),'xgb':XGBClassifier(),'sv':svm.SVC(),'nb':naive_bayes.GaussianNB(),'mlp':neural_network.MLPClassifier()} for model in models: models[model].fit(x_train.copy(),y_train.copy()) y_pred = models[model].predict(x_test.copy()) score_table['Accuracy Score'].append(metrics.accuracy_score(y_test.copy(),y_pred)) score_table['ROC_AUC'].append(metrics.roc_auc_score(y_test.copy(),y_pred)) score_table['F1'].append(metrics.f1_score(y_test.copy(),y_pred)) return pd.DataFrame(score_table) compare_models(cleanIt.fit_transform(x_train),cleanIt.transform(x_test),y_train,y_test) There are more models to explore, you can simply put in the list of models and make it instance. You're good to go. ADABoost classifier is giving us great accuracy, roc_auc score and f1 score, so we'll \u25b6\ufe0f with this. View on Github","title":"Model Selection"},{"location":"modelSelectR/","text":"Let's choose the model from different type of regresion models \ud83d\udd05 Import the libraries, read the data and previous discussed stuff # Importing essential libraries import pandas as pd import joblib as jb from sklearn import preprocessing, metrics, impute, model_selection, compose, linear_model, tree, svm, ensemble # Reading data df = pd.read_csv('data.csv') df.drop('car_ID',axis=1,inplace=True) # Split in feature and target for future purpose features = df.drop(['CarName','price'],axis=1) target = df.price # Names of numerical and categorical features numerical_features = [col for col in features.columns if features[col].dtypes!='O'] categorical_features = [col for col in features.columns if col not in numerical_features] # Train test split x_train,x_test,y_train,y_test = model_selection.train_test_split(features,target,random_state=32) # Nominal features nominal_features = ['fueltype','aspiration','doornumber','carbody','drivewheel','enginelocation','enginetype','cylindernumber','fuelsystem'] Load the data transformation pipeline we've created in previous slides cleaning_pipeline_with_scaler = jb.load('cleanpipeline1.jb') cleaning_pipeline_witthout_scaler = jb.load('cleanpipeline2.jb') Now we're using that same function we've discussed in classification section \ud83d\udd17 # With scaling included def compare_models_notree(x_train,x_test,y_train,y_test): score_table = { 'Classifiers': ['Linear Regression','Ridge Regression','Lasso Regression','Support Vector Regression','Huber Regression'], 'r_square':[], 'mae':[], 'mse': [] } models = {'Linreg':linear_model.LinearRegression(),'ridge':linear_model.Ridge(),'lasso':linear_model.Lasso(),'svr':svm.SVR(),'huber':linear_model.HuberRegressor()} for model in models: models[model].fit(x_train.copy(),y_train.copy()) y_pred = models[model].predict(x_test.copy()) score_table['r_square'].append(metrics.r2_score(y_test.copy(),y_pred)) score_table['mae'].append(metrics.mean_absolute_error(y_test.copy(),y_pred)) score_table['mse'].append(metrics.mean_squared_error(y_test.copy(),y_pred)) return pd.DataFrame(score_table) compare_models_notree(cleaning_pipeline_with_scaler.fit_transform(x_train),cleaning_pipeline_with_scaler.transform(x_test),y_train,y_test) # With scaling excluded def compare_models_tree(x_train,x_test,y_train,y_test): score_table = { 'Classifiers': ['Decision Tree','Random Forest','ADABoost'], 'r_square':[], 'mae':[], 'mse': [] } models = {'dt':tree.DecisionTreeRegressor(),'rf':ensemble.RandomForestRegressor(),'ada':ensemble.AdaBoostRegressor()} for model in models: models[model].fit(x_train.copy(),y_train.copy()) y_pred = models[model].predict(x_test.copy()) score_table['r_square'].append(metrics.r2_score(y_test.copy(),y_pred)) score_table['mae'].append(metrics.mean_absolute_error(y_test.copy(),y_pred)) score_table['mse'].append(metrics.mean_squared_error(y_test.copy(),y_pred)) return pd.DataFrame(score_table) compare_models_tree(cleaning_pipeline_with_scaler.fit_transform(x_train),cleaning_pipeline_with_scaler.transform(x_test),y_train,y_test) Boooom , tree regressor is winning by the way i.e. Random Forest Regression View on Github","title":"Model Selection"},{"location":"scraping/","text":"Let's proceed to collect or scrap the data. Scraping data can be easily understood as grabbing or extracting some information from websites or application that are useful for Data Scientist or Machine Learning Engineers to analyze the problem statement. No more talking in air now, let's code <> # Importing Essential libraries import pandas as pd import requests from bs4 import BeautifulSoup as bs pandas - This library is useful for reading, transforming and analyzing the tabular data like csv ,xlsx etc. requests - This library is used for fetching the HTML content using link which is passed to it. bs4 - Popularly known as beautiful soup used to extract the data by filtering some tags present in HTML. # Create some lists name_of_movie = [] ranking = [] year_of_release = [] ratings = [] Here we've created some list, which is used to store the extracted data in a csv format. try: source = requests.get('https://www.imdb.com/chart/top/') source.raise_for_status() soup = bs(source.text,'html.parser') movies = soup.find('tbody',class_='lister-list').find_all('tr') for movie in movies: name_of_movie.append(movie.find('td',class_='titleColumn').a.text) ranking.append(movie.find('td',class_='titleColumn').get_text(strip=True).split('.')[0]) year_of_release.append(movie.find('td',class_='titleColumn').span.text.strip('()')) ratings.append(movie.find('td',class_='ratingColumn imdbRating').strong.text) except Exception as e: print(e) Now let's understand functions used in each line of code requests.get - Used to fetch the content which we can get through by going to it (HTML content). Just go to this url , right click and inspect, therefore the html code which you've seen it is stored in source variable. raise_for_status - It is just for confirmation, that code is extracted or not. If not, it'll raise an error. bs - It takes two arguments (and many optional), one is content (.text is returning the content) and another is which type of content bs function need to parse or analyze, that is html parser. There are many parser available, you can check here . To understand the working of beautiful soup, you would've a brief understanding of HTML tags. soup.find - As we know, HTML is full of tags, now assume we need to find the title of picture,rating of picture and its ranking, so we need to find the parent tag inside which name, ranking and rating is plugged. In our case, inside tbody tag having class name lister-list , we have our all content. To get movies, every tr tag inside tbody tag has movies (and its all info). Purpose of these function is to return(in movies variable) the content of that tag specified having specified class name. In next line, we'll iterating more inside movies, to get each movies title, rating and ranking. [**tbody(lister-list) -> tr -> td(titleColumns) -> a**] -> movie names [**tbody(lister-list) -> tr -> td(titleColumns)] -> ranking (using split) [**tbody(lister-list) -> tr -> td(ratingColumn imdbRating) -> strong**] -> ranking We've stored the movie title,ranking,year,ratings is variable name_of_movie , ranking , year_of_release , ratings respectively. data = pd.DataFrame({'Rank':ranking,'Movie':name_of_movie,'YearOfRelease':year_of_release,'Ratings':ratings}) data.head(10) Simply append the lists that we've created in a Dataframe. Output(1st 10 tuples) - Rank Movie YearOfRelease Ratings 1 The Shawshank Redemption 1994 9.2 2 The Godfather 1972 9.2 3 The Dark Knight 2008 9.0 4 The Godfather Part II 1974 9.0 5 12 Angry Men 1957 8.9 6 Schindler's List 1993 8.9 7 The Lord of the Rings: The Return of the King 2003 8.9 8 Pulp Fiction 1994 8.9 9 The Lord of the Rings: The Fellowship of the Ring 2001 8.8 10 Il buono, il brutto, il cattivo 1966 8.8 You can apply this type of procedure with little variation in any web scraping task. View on Github","title":"Data Collection"},{"location":"scraping/#lets-proceed-to-collect-or-scrap-the-data","text":"Scraping data can be easily understood as grabbing or extracting some information from websites or application that are useful for Data Scientist or Machine Learning Engineers to analyze the problem statement. No more talking in air now, let's code <> # Importing Essential libraries import pandas as pd import requests from bs4 import BeautifulSoup as bs pandas - This library is useful for reading, transforming and analyzing the tabular data like csv ,xlsx etc. requests - This library is used for fetching the HTML content using link which is passed to it. bs4 - Popularly known as beautiful soup used to extract the data by filtering some tags present in HTML. # Create some lists name_of_movie = [] ranking = [] year_of_release = [] ratings = [] Here we've created some list, which is used to store the extracted data in a csv format. try: source = requests.get('https://www.imdb.com/chart/top/') source.raise_for_status() soup = bs(source.text,'html.parser') movies = soup.find('tbody',class_='lister-list').find_all('tr') for movie in movies: name_of_movie.append(movie.find('td',class_='titleColumn').a.text) ranking.append(movie.find('td',class_='titleColumn').get_text(strip=True).split('.')[0]) year_of_release.append(movie.find('td',class_='titleColumn').span.text.strip('()')) ratings.append(movie.find('td',class_='ratingColumn imdbRating').strong.text) except Exception as e: print(e) Now let's understand functions used in each line of code requests.get - Used to fetch the content which we can get through by going to it (HTML content). Just go to this url , right click and inspect, therefore the html code which you've seen it is stored in source variable. raise_for_status - It is just for confirmation, that code is extracted or not. If not, it'll raise an error. bs - It takes two arguments (and many optional), one is content (.text is returning the content) and another is which type of content bs function need to parse or analyze, that is html parser. There are many parser available, you can check here . To understand the working of beautiful soup, you would've a brief understanding of HTML tags. soup.find - As we know, HTML is full of tags, now assume we need to find the title of picture,rating of picture and its ranking, so we need to find the parent tag inside which name, ranking and rating is plugged. In our case, inside tbody tag having class name lister-list , we have our all content. To get movies, every tr tag inside tbody tag has movies (and its all info). Purpose of these function is to return(in movies variable) the content of that tag specified having specified class name. In next line, we'll iterating more inside movies, to get each movies title, rating and ranking. [**tbody(lister-list) -> tr -> td(titleColumns) -> a**] -> movie names [**tbody(lister-list) -> tr -> td(titleColumns)] -> ranking (using split) [**tbody(lister-list) -> tr -> td(ratingColumn imdbRating) -> strong**] -> ranking We've stored the movie title,ranking,year,ratings is variable name_of_movie , ranking , year_of_release , ratings respectively. data = pd.DataFrame({'Rank':ranking,'Movie':name_of_movie,'YearOfRelease':year_of_release,'Ratings':ratings}) data.head(10) Simply append the lists that we've created in a Dataframe. Output(1st 10 tuples) - Rank Movie YearOfRelease Ratings 1 The Shawshank Redemption 1994 9.2 2 The Godfather 1972 9.2 3 The Dark Knight 2008 9.0 4 The Godfather Part II 1974 9.0 5 12 Angry Men 1957 8.9 6 Schindler's List 1993 8.9 7 The Lord of the Rings: The Return of the King 2003 8.9 8 Pulp Fiction 1994 8.9 9 The Lord of the Rings: The Fellowship of the Ring 2001 8.8 10 Il buono, il brutto, il cattivo 1966 8.8 You can apply this type of procedure with little variation in any web scraping task. View on Github","title":"Let's proceed to collect or scrap the data."},{"location":"tuningC/","text":"We are choosing ADA Boost classifier, so let's make it more accurate using the hyperparameters. \ud83d\udd0f #Importing essential libraries import pandas as pd import joblib as jb from sklearn import ensemble, pipeline, compose, model_selection # Read ,split the data and import the cleaning pipeline df = pd.read_csv('spaceTrain.csv') # removing useless columns features = df.drop(['PassengerId','Cabin','Name','Transported'],axis = 1) target = df.Transported # Names of Numerical and Categorical features numerical_features = [feat for feat in features if features[feat].dtypes !='O'] categorical_features = [feat for feat in features if feat not in numerical_features] # Split your dataset into training and testing dataset x_train,x_test,y_train,y_test = model_selection.train_test_split(features,target,test_size = 0.2,stratify=target) # Importing pipeline cleanIt = jb.load('clean.joblib') Hyperparameters \ud83d\udcd1 of ADA Boost classifier - Hyperparameters Description base_estimator The base estimator from which the boosted ensemble is built. n_estimators The maximum number of estimators at which boosting is terminated. I learning_rate Weight applied to each classifier at each boosting iteration. A higher learning rate increases the contribution of each classifier. algorithm Two algorithms based on which boosting works random_state Controls the random seed given at each base_estimator at each boosting iteration. Now let's create a list of parameters and fit it in grid search module parameters = { 'n_estimators': [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 20], 'learning_rate': [(0.97 + x / 100) for x in range(0, 8)], 'algorithm': ['SAMME', 'SAMME.R'] } clf = ensemble.AdaBoostClassifier() grid = model_selection.GridSearchCV(clf,parameters,cv=5,verbose=0) grid.fit(cleanIt.fit_transform(x_train),y_train) To find the best estimator, best score and best parameters grid.best_estimator_,grid.best_params_,grid.best_score_ Create the final model with best parameters model = ensemble.AdaBoostClassifier(n_estimators=20,learning_rate=1.04,algorithm='SAMME.R') Final pipeline Final_pipeline = pipeline.make_pipeline(cleanIt,model) Final_pipeline.fit(x_train,y_train) y_pred = Final_pipeline.predict(x_test) Check the accuracies accuracy = metrics.accuracy_score(y_test,y_pred) roc_auc = metrics.roc_auc_score(y_test,y_pred) f1score = metrics.f1_score(y_test,y_pred) With this, we've completed our classification algorithm \ud83c\udf88 View on Github","title":"Hyperparameter Tuning"},{"location":"tuningC/#hyperparameters-of-ada-boost-classifier-","text":"Hyperparameters Description base_estimator The base estimator from which the boosted ensemble is built. n_estimators The maximum number of estimators at which boosting is terminated. I learning_rate Weight applied to each classifier at each boosting iteration. A higher learning rate increases the contribution of each classifier. algorithm Two algorithms based on which boosting works random_state Controls the random seed given at each base_estimator at each boosting iteration. Now let's create a list of parameters and fit it in grid search module parameters = { 'n_estimators': [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 20], 'learning_rate': [(0.97 + x / 100) for x in range(0, 8)], 'algorithm': ['SAMME', 'SAMME.R'] } clf = ensemble.AdaBoostClassifier() grid = model_selection.GridSearchCV(clf,parameters,cv=5,verbose=0) grid.fit(cleanIt.fit_transform(x_train),y_train) To find the best estimator, best score and best parameters grid.best_estimator_,grid.best_params_,grid.best_score_ Create the final model with best parameters model = ensemble.AdaBoostClassifier(n_estimators=20,learning_rate=1.04,algorithm='SAMME.R') Final pipeline Final_pipeline = pipeline.make_pipeline(cleanIt,model) Final_pipeline.fit(x_train,y_train) y_pred = Final_pipeline.predict(x_test) Check the accuracies accuracy = metrics.accuracy_score(y_test,y_pred) roc_auc = metrics.roc_auc_score(y_test,y_pred) f1score = metrics.f1_score(y_test,y_pred)","title":"Hyperparameters \ud83d\udcd1 of ADA Boost classifier -"},{"location":"tuningC/#with-this-weve-completed-our-classification-algorithm","text":"View on Github","title":"With this, we've completed our classification algorithm \ud83c\udf88"},{"location":"tuningR/","text":"Now let's tune our Random Forest Regressor and create a final model pipeline \ud83c\udf88 # Importing essential libraries import pandas as pd import joblib as jb from sklearn import preprocessing, metrics, impute, model_selection, compose, ensemble # Reading data df = pd.read_csv('data.csv') df.drop('car_ID',axis=1,inplace=True) # Split in feature and target for future purpose features = df.drop(['CarName','price'],axis=1) target = df.price # Names of numerical and categorical features numerical_features = [col for col in features.columns if features[col].dtypes!='O'] categorical_features = [col for col in features.columns if col not in numerical_features] # Train test split x_train,x_test,y_train,y_test = model_selection.train_test_split(features,target,random_state=32) # Nominal features nominal_features = ['fueltype','aspiration','doornumber','carbody','drivewheel','enginelocation','enginetype','cylindernumber','fuelsystem'] # Import the pipeline with scaling excluded because we're using tree based model cleaning_pipeline_witthout_scaler = jb.load('cleanpipeline2.jb') Create a list of parameters which is used to tune the random forest regressor Parameters Description n_estimators The number of trees in the forest. criterion The function to measure the quality of a split. max_depth The maximum depth of the tree. min_samples_split The minimum number of samples required to split an internal node min_samples_leaf The minimum number of samples required to be at a leaf node. min_weight_fraction_leaf The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. max_features The number of features to consider when looking for the best split: max_leaf_nodes Grow trees with max_leaf_nodes in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes. min_impurity_decrease A node will be split if this split induces a decrease of the impurity greater than or equal to this value. bootstrap Whether bootstrap samples are used when building trees. If False, the whole dataset is used to build each tree. oob_score Whether to use out-of-bag samples to estimate the generalization score. n_jobs The number of jobs to run in parallel. random_state Controls both the randomness of the bootstrapping of the samples used when building trees verbose Controls the verbosity when fitting and predicting. warm_state When set to True, reuse the solution of the previous call to fit and add more estimators to the ensemble, otherwise, just fit a whole new forest. ccp_alpha Complexity parameter used for Minimal Cost-Complexity Pruning. max_samples If bootstrap is True, the number of samples to draw from X to train each base estimator. params = { 'n_estimators' : [int(x) for x in np.linspace(start = 100, stop = 500, num = 5)], 'max_features' : ['auto', 'sqrt'], 'max_depth' : [int(x) for x in np.linspace(5, 30, num = 6)], 'min_samples_split' : [2, 5, 10, 15, 100], 'min_samples_leaf' : [1, 2, 5, 10] } model = ensemble.RandomForestRegressor() randomizedcv = model_selection.RandomizedSearchCV(model,params) randomizedcv.fit(cleaning_pipeline_witthout_scaler.fit_transform(x_train),y_train) Know about best estimator, parameters and score # Best parameterized estimator, Optimal parameters, Score achieved after applying optimal parameters randomizedcv.best_estimator_, randomizedcv.best_params_, randomizedcv.best_score_ Final Pipeline # Transformer used to encode the categorical column encode = compose.make_column_transformer( (preprocessing.OneHotEncoder(drop = 'first',handle_unknown='ignore'),categorical_features), remainder = 'passthrough' ) # Optimal parameterized random forest regressor model = ensemble.RandomForestRegressor(n_estimators=500,min_samples_split=5,min_samples_leaf=1,max_features='sqrt',max_depth=20) final_pipeline = pipeline.make_pipeline(encode,model) final_pipeline.fit(x_train,y_train) final_pipeline.predict(x_test) View on Github","title":"Hyperparameter Tuning"},{"location":"types/","text":"About Data Machine learning model's fuel is data. There can many types of data through which we can find relations like Numerical Data (like [23,45,1,22,3,0056] etc) Textual / Categorical Data (like desinations ['CEO','Assistant Manager'] etc) Image Data in form of pixels (like img = [[0,1,0,34,655.....],[34,56,1,1....]] etc) Video format Data (Here we deal videos as a group of millions of images) Voice Data (like frequency, amplitude) Hybrid data (Mixture of everything) If we take about data, we have its significant part which is Independent Variables and Dependent Variables . Independent Variables - These are the values on which machine learning model finds the relations and train and predict what ??. Dependent Variables - These are the values which varies according to independent variables i.e. they depends on independent variables. For example :- There is a data in which machine has to learn through data that it is male or female. S.no. hair_length(in cm) height(in cm) voice_pitch(in hz) gender 1 20 155 10 'Female' 2 4 140 28 'Male' 3 8 150 15 'Male' Here, hair_length , height , voice_pitch are the independent variables and gender is the dependent variable. Here independent variable can also called Features and dependent variable to be target . Types Of ML problems As we to learn machine needs data, but data can be in two forms i.e. * Data with target variable (Supervised Learning) * Data without target variable (Unsupervised Learning) When we've to create the ml model based on data which have output / target, it's somewhat easy to create as compared to data without target. Supervised Learning In this type of learning, presence of target in data is mandatory. Let's take two examples on which we further classify supervised learning. Example no.1 - Assume there are patients in an hospital, and our model needs to predict their blood pressure rate according to given data. Data - Activity_hours Sweet_consumption Is_depressed BMI Sex Blood_pressure 2 2 0 79 'M' 90 3 3 1 88 'F' 145 1 4 1 102 'M' 130 5 5 0 100 'M' 100 Here Blood_pressure is the target variable. We can see that value in target variable is kinda continuous it can be anything like 90 ,91,102.5,107, it's not a discrete like only falls in 20,30,40 for example. So whenever we encounter the ml problems in which we have to predict the value which is continuous in nature, we can call them Regression Problem . Example no. 2 - Lets extend the above dataset and our machine learning model wants to predict that the patient has thyroid or not. Data - Activity_hours Sweet_consumption Is_depressed BMI Sex Blood_pressure has_thyroid 2 2 0 79 'M' 90 0 3 3 1 88 'F' 145 1 1 4 1 102 'M' 130 1 5 5 0 100 'M' 100 0 Here has_thyroid is the target variable. We can see that we only have to predict that patient has thyroid or not i.e. 0 means no thyroid and 1 means has thyroid. So whenever we encounter ml problems in which we have to predict the value which is discrete in nature, we call them Classification Problem . Unsupervised Learning Now, you are playing blindly in machine learning game, you've given the data which have no label. So what we have to predict ??? That's why supervised learning is quite doable as compared with unsupervised learning. In unsupervised learning, we have the data and should try to combine the sub-part of data which have some similar relations which we called classes. So to separate our data in some classes (randomly names class 1,2 etc) is the phenomena of Clustering . Example - Assume we have given data in form of images of dogs,cats and cows. Data - \ud83d\udc2e \ud83d\udc15 \ud83d\udc31 \ud83d\udc2e \ud83d\udc31 \ud83d\udc15 \ud83d\udc15 \ud83d\udc31 Now ,our machine learning model don't know what cow,dog or cat is, and even data doesn't have target. So it separates our data into classes(concept of clustering) which it names as 0,1,2 where 0 classifies images of dogs \ud83d\udc15 \ud83d\udc15 \ud83d\udc15 1 classifies images of cats \ud83d\udc31 \ud83d\udc31 2 classifies images of cows \ud83d\udc2e Hope you understand the types of machine learning tasks !!!","title":"Types of ML task"},{"location":"types/#about-data","text":"Machine learning model's fuel is data. There can many types of data through which we can find relations like Numerical Data (like [23,45,1,22,3,0056] etc) Textual / Categorical Data (like desinations ['CEO','Assistant Manager'] etc) Image Data in form of pixels (like img = [[0,1,0,34,655.....],[34,56,1,1....]] etc) Video format Data (Here we deal videos as a group of millions of images) Voice Data (like frequency, amplitude) Hybrid data (Mixture of everything) If we take about data, we have its significant part which is Independent Variables and Dependent Variables . Independent Variables - These are the values on which machine learning model finds the relations and train and predict what ??. Dependent Variables - These are the values which varies according to independent variables i.e. they depends on independent variables. For example :- There is a data in which machine has to learn through data that it is male or female. S.no. hair_length(in cm) height(in cm) voice_pitch(in hz) gender 1 20 155 10 'Female' 2 4 140 28 'Male' 3 8 150 15 'Male' Here, hair_length , height , voice_pitch are the independent variables and gender is the dependent variable. Here independent variable can also called Features and dependent variable to be target .","title":"About Data"},{"location":"types/#types-of-ml-problems","text":"As we to learn machine needs data, but data can be in two forms i.e. * Data with target variable (Supervised Learning) * Data without target variable (Unsupervised Learning) When we've to create the ml model based on data which have output / target, it's somewhat easy to create as compared to data without target.","title":"Types Of ML problems"},{"location":"types/#supervised-learning","text":"In this type of learning, presence of target in data is mandatory. Let's take two examples on which we further classify supervised learning. Example no.1 - Assume there are patients in an hospital, and our model needs to predict their blood pressure rate according to given data. Data - Activity_hours Sweet_consumption Is_depressed BMI Sex Blood_pressure 2 2 0 79 'M' 90 3 3 1 88 'F' 145 1 4 1 102 'M' 130 5 5 0 100 'M' 100 Here Blood_pressure is the target variable. We can see that value in target variable is kinda continuous it can be anything like 90 ,91,102.5,107, it's not a discrete like only falls in 20,30,40 for example. So whenever we encounter the ml problems in which we have to predict the value which is continuous in nature, we can call them Regression Problem . Example no. 2 - Lets extend the above dataset and our machine learning model wants to predict that the patient has thyroid or not. Data - Activity_hours Sweet_consumption Is_depressed BMI Sex Blood_pressure has_thyroid 2 2 0 79 'M' 90 0 3 3 1 88 'F' 145 1 1 4 1 102 'M' 130 1 5 5 0 100 'M' 100 0 Here has_thyroid is the target variable. We can see that we only have to predict that patient has thyroid or not i.e. 0 means no thyroid and 1 means has thyroid. So whenever we encounter ml problems in which we have to predict the value which is discrete in nature, we call them Classification Problem .","title":"Supervised Learning"},{"location":"types/#unsupervised-learning","text":"Now, you are playing blindly in machine learning game, you've given the data which have no label. So what we have to predict ??? That's why supervised learning is quite doable as compared with unsupervised learning. In unsupervised learning, we have the data and should try to combine the sub-part of data which have some similar relations which we called classes. So to separate our data in some classes (randomly names class 1,2 etc) is the phenomena of Clustering . Example - Assume we have given data in form of images of dogs,cats and cows. Data - \ud83d\udc2e \ud83d\udc15 \ud83d\udc31 \ud83d\udc2e \ud83d\udc31 \ud83d\udc15 \ud83d\udc15 \ud83d\udc31 Now ,our machine learning model don't know what cow,dog or cat is, and even data doesn't have target. So it separates our data into classes(concept of clustering) which it names as 0,1,2 where 0 classifies images of dogs \ud83d\udc15 \ud83d\udc15 \ud83d\udc15 1 classifies images of cats \ud83d\udc31 \ud83d\udc31 2 classifies images of cows \ud83d\udc2e Hope you understand the types of machine learning tasks !!!","title":"Unsupervised Learning"},{"location":"what/","text":"What actually machine learning is ?? Let's take some example for understand the concept of machine learning. Example no. 1 - Assume you are preparing for competitive exam (or maybe goverment), and you exactly dont know about the question and its patterns. You can prepare for exam using \ud83d\udcda,coaching or online resource like \ud83c\udf10. Therefore what you're really trying to train your mind through studying to know all types of pattern that can occurs. Finally in exam , you'll see the question and match the patterns that your mind have captured with it then solve it. Am i right ??? Example no. 2 - Assume you are the owner of renowned \ud83c\udf66 company and going through the downfall of your company. So you'll try to analyze what's going on wrong ?? Is our high range of flavours confusing customers and eventually they chooses wrong taste. Are they bored with our flavours. Is our price of ice creams are high. It can also depend on behavior of our employers to customers. and many more. Basically you have all this data through cctv cameras, survey forms etc and you'll find some relations between customers satisfaction and your services. Am i right ?? That's what ML is for !! \ud83d\udc4d Machine learning is a model or in leymann kinda box, which takes input data and gives the result which we really want by finding the relation between input data. Look at some more examples here .","title":"What is ML"},{"location":"what/#what-actually-machine-learning-is","text":"Let's take some example for understand the concept of machine learning. Example no. 1 - Assume you are preparing for competitive exam (or maybe goverment), and you exactly dont know about the question and its patterns. You can prepare for exam using \ud83d\udcda,coaching or online resource like \ud83c\udf10. Therefore what you're really trying to train your mind through studying to know all types of pattern that can occurs. Finally in exam , you'll see the question and match the patterns that your mind have captured with it then solve it. Am i right ??? Example no. 2 - Assume you are the owner of renowned \ud83c\udf66 company and going through the downfall of your company. So you'll try to analyze what's going on wrong ?? Is our high range of flavours confusing customers and eventually they chooses wrong taste. Are they bored with our flavours. Is our price of ice creams are high. It can also depend on behavior of our employers to customers. and many more. Basically you have all this data through cctv cameras, survey forms etc and you'll find some relations between customers satisfaction and your services. Am i right ??","title":"What actually machine learning is ??"},{"location":"what/#thats-what-ml-is-for","text":"Machine learning is a model or in leymann kinda box, which takes input data and gives the result which we really want by finding the relation between input data. Look at some more examples here .","title":"That's what ML is for !! \ud83d\udc4d"},{"location":"workflow/","text":"Workflow for creating machine learning model \ud83c\udfd7\ufe0f To create any machine learning models, we follow some core steps to reach our goal. As we've previously workflow in introduction section . Here we'll discuss in depth. Step 1\ufe0f\u20e3 Define the problem - When any problem comes to you (like identify / predict this, that whatever). Just think !!! What we really have to find/predict. Is your problem classification or regression or is it a unsupervised. Is your predictions gonna benefits your company. What data or feature you require to creating the model. Step 2\ufe0f\u20e3 Data Collection - After defining the problem statement, sometimes should be given to you if you are blessed by GOD otherwise you have to scrap the data from website. After collecting, Just think \ud83e\udd14!!! Does that data defines or suits your problem efficiently or more independent variables are required. There are many methods or we can say python libraries used to collect the data from website (also known as web scraping) like Beautiful Soup Requests Selenium Urllib and many more....... Step 3\ufe0f\u20e3 Data Exploration \ud83d\udca5 - After collecting data successfully, you have to find out what that data is telling to you like How much rows or columns it has. What are the counts of values in a columns, is it columns biased (means only one type of value is filled in whole column). Are there any missing values and how you deal with it. Are there any outliers(some data points which are far different from all points like [1,2,3,10,100], here 100 is outlier). Is the distribution of feature is normal(using graph, we check is distribution is gaussian or not) or kinda biased(not following any pattern). As our machine learning model doesn't accept categorical/textual data, how you can convert it into numerical form. Check relationship between each indepenent feature with independent feature and also independent feature with dependent feature (There should be no relation between independent and independent otherwise non-collinearity occurs and there should be some relation between independent and dependent). There is lot of more steps to clean. Step 4\ufe0f\u20e3 Splitting \ud83d\udee3\ufe0f - After exploration of dataset, just split your data in part before doing any futher steps. You can split your data in many parts such as - Train-Test split - In this ,keep 80%(generally) of data into training of your model and test your model in remaining 20% of your data. Cross validation - In this, divide your data in more than 2 parts(usually) like assume 5. So therefore keep the first 4 parts in training and last one in testing, then again train with different 4 parts and anyone part in testing. There are many cross validation techniques such as leave-one-out , k-fold , stratified-k-fold , Hold-out and Repeated-k-fold . Note - After splitting, whatever cleaning, modellin we have to done, do it with training data and keep testing data aside until you want to test your model . Step 5\ufe0f\u20e3 Data Cleaning \ud83e\uddf9 - After splitting the dataset(or you can call it data), you have to get your hands dirty in cleaning the data because more cleaner the data you feed to your model, more accurate your model becomes. Steps involved in cleaning are :- Imputing the missing(null) values by removing them or replacing them with its column's mean,median or mode depends on situation. (There are also multivariate imputing concept, we'll discuss it). Encode the categorical features means convert categorical features in numerical features. There are many to encode the categorical like ordinal encoding , one hot encoding , target encoding . There may be outliers in the data, so check it using boxplot graph. If there are outliers,we should remove it if we've have lots of data otherwise we shouldn't. There are many methods to remove or replace the outlier's values such as trimming , capping , inter-quartile-range and discretization . If it is a regression problem or any neural network problem, we also have to make our standardized or normalized (all values should lie between particular range). We can do this using standardization or normalization.[Feature Scaling] Standardization or normalization process doesn't required in classification task. Making the distribution gaussian by using log ,exponential, square or else-other. Remove the feature which are related with independent feature and not related with target. We can identify this using correlation map and can remove feature using some methods such as pearson correlation coefficient , anova test , chi square test , selectKbest , selectpercentil etc. Most Most Most important step is to create a pipeline for data cleaning, so that you don't to do same and same task by your own. Pipeline can do its work automatically. Don't hesistate to take time to explore and clean data. Almost 80% of work in creating machine learning model is the feature engineering[all parts that we discussed above] . Step 6\ufe0f\u20e3 Model Building \ud83c\udfda\ufe0f - After cleaning the data and creating its pipeline, now we are good to go with model building. Model building consist of following steps such as - Different types of algorithm are there in python libraries, which helps us to create ml models. List of few of them - Regression & Classification Models Linear Regression (regression) Stochastic Gradient Descent (For both,but mainly regression) Logistic Regression (classification) Support Vector Machine (For both) Decision Tree (For both) Random Forest (For both) Naive Bayes(For classification) ADABoost and Gradient Boost (For classification) KNearest Neighbor (For both) Never ending list......... To know more about this models, you can visit sklearn . Model creation is as easy as creating instance using function. But we have to make our model accurate so for this Step 7\ufe0f\u20e3 appears. Model selection is really crucial, and you also have to try each model based on classification or regression. Step 7\ufe0f\u20e3 Tuning and tweaking - To create highly accurate model, we want to choose the correct model from lots of options and that models also contains parameters (functional parameters), so we have to select the right parameters for our model and this process is called Hyperparameter tuning with model selection . If you are confused here, don't worry we'll see a code example in furhter pages . Step 8\ufe0f\u20e3 Evaluation \ud83d\udcd2 - After creating model, we are choosing hyperparameter right ??. So how we can confirm that these are right parameters, by checking accuracy. Accuracy is a method in which compare predicted values with real target values. ''More our values are matching with true values, more accurate our model is''. To evaluate, we use metrics (its just like methods to find accuracy), for classification and regression we use different metrics such as |Regression|Classification| |----------|--------------| |Mean Absolute Error|Accuracy| |Mean Squared Error|Precision| |Root Mean Squared Error|F1 score| |Root Mean Logarithmic Error|ROC curve| |Mean Percentage Error|AUC| |Mean Absolute Percentage Error|Log loss| |R square|Average precision| ||Mean Average Precision| ||Confusion Matrix| To know more about evaluation metrics, check sklearn here . After creating the model for high accuracy, don't forget to put it in pipeline Step 9\ufe0f\u20e3 Test on real world - After model building, test your model on testing set(unknown data). Feed your data to pipeline and pipeline do all the feature engineering and model testing by its own. Key points to remember \ud83d\udd8a\ufe0f Machine learning is an iterative process, more we see data, more our model will learn and more accurate it will become.","title":"Workflow of ML model"},{"location":"workflow/#workflow-for-creating-machine-learning-model","text":"To create any machine learning models, we follow some core steps to reach our goal. As we've previously workflow in introduction section . Here we'll discuss in depth.","title":"Workflow for creating machine learning model \ud83c\udfd7\ufe0f"},{"location":"workflow/#step-1","text":"Define the problem - When any problem comes to you (like identify / predict this, that whatever). Just think !!! What we really have to find/predict. Is your problem classification or regression or is it a unsupervised. Is your predictions gonna benefits your company. What data or feature you require to creating the model.","title":"Step 1\ufe0f\u20e3"},{"location":"workflow/#step-2","text":"Data Collection - After defining the problem statement, sometimes should be given to you if you are blessed by GOD otherwise you have to scrap the data from website. After collecting, Just think \ud83e\udd14!!! Does that data defines or suits your problem efficiently or more independent variables are required. There are many methods or we can say python libraries used to collect the data from website (also known as web scraping) like Beautiful Soup Requests Selenium Urllib and many more.......","title":"Step 2\ufe0f\u20e3"},{"location":"workflow/#step-3","text":"Data Exploration \ud83d\udca5 - After collecting data successfully, you have to find out what that data is telling to you like How much rows or columns it has. What are the counts of values in a columns, is it columns biased (means only one type of value is filled in whole column). Are there any missing values and how you deal with it. Are there any outliers(some data points which are far different from all points like [1,2,3,10,100], here 100 is outlier). Is the distribution of feature is normal(using graph, we check is distribution is gaussian or not) or kinda biased(not following any pattern). As our machine learning model doesn't accept categorical/textual data, how you can convert it into numerical form. Check relationship between each indepenent feature with independent feature and also independent feature with dependent feature (There should be no relation between independent and independent otherwise non-collinearity occurs and there should be some relation between independent and dependent). There is lot of more steps to clean.","title":"Step 3\ufe0f\u20e3"},{"location":"workflow/#step-4","text":"Splitting \ud83d\udee3\ufe0f - After exploration of dataset, just split your data in part before doing any futher steps. You can split your data in many parts such as - Train-Test split - In this ,keep 80%(generally) of data into training of your model and test your model in remaining 20% of your data. Cross validation - In this, divide your data in more than 2 parts(usually) like assume 5. So therefore keep the first 4 parts in training and last one in testing, then again train with different 4 parts and anyone part in testing. There are many cross validation techniques such as leave-one-out , k-fold , stratified-k-fold , Hold-out and Repeated-k-fold . Note - After splitting, whatever cleaning, modellin we have to done, do it with training data and keep testing data aside until you want to test your model .","title":"Step 4\ufe0f\u20e3"},{"location":"workflow/#step-5","text":"Data Cleaning \ud83e\uddf9 - After splitting the dataset(or you can call it data), you have to get your hands dirty in cleaning the data because more cleaner the data you feed to your model, more accurate your model becomes. Steps involved in cleaning are :- Imputing the missing(null) values by removing them or replacing them with its column's mean,median or mode depends on situation. (There are also multivariate imputing concept, we'll discuss it). Encode the categorical features means convert categorical features in numerical features. There are many to encode the categorical like ordinal encoding , one hot encoding , target encoding . There may be outliers in the data, so check it using boxplot graph. If there are outliers,we should remove it if we've have lots of data otherwise we shouldn't. There are many methods to remove or replace the outlier's values such as trimming , capping , inter-quartile-range and discretization . If it is a regression problem or any neural network problem, we also have to make our standardized or normalized (all values should lie between particular range). We can do this using standardization or normalization.[Feature Scaling] Standardization or normalization process doesn't required in classification task. Making the distribution gaussian by using log ,exponential, square or else-other. Remove the feature which are related with independent feature and not related with target. We can identify this using correlation map and can remove feature using some methods such as pearson correlation coefficient , anova test , chi square test , selectKbest , selectpercentil etc. Most Most Most important step is to create a pipeline for data cleaning, so that you don't to do same and same task by your own. Pipeline can do its work automatically. Don't hesistate to take time to explore and clean data. Almost 80% of work in creating machine learning model is the feature engineering[all parts that we discussed above] .","title":"Step 5\ufe0f\u20e3"},{"location":"workflow/#step-6","text":"Model Building \ud83c\udfda\ufe0f - After cleaning the data and creating its pipeline, now we are good to go with model building. Model building consist of following steps such as - Different types of algorithm are there in python libraries, which helps us to create ml models. List of few of them - Regression & Classification Models Linear Regression (regression) Stochastic Gradient Descent (For both,but mainly regression) Logistic Regression (classification) Support Vector Machine (For both) Decision Tree (For both) Random Forest (For both) Naive Bayes(For classification) ADABoost and Gradient Boost (For classification) KNearest Neighbor (For both) Never ending list......... To know more about this models, you can visit sklearn . Model creation is as easy as creating instance using function. But we have to make our model accurate so for this Step 7\ufe0f\u20e3 appears. Model selection is really crucial, and you also have to try each model based on classification or regression.","title":"Step 6\ufe0f\u20e3"},{"location":"workflow/#step-7","text":"Tuning and tweaking - To create highly accurate model, we want to choose the correct model from lots of options and that models also contains parameters (functional parameters), so we have to select the right parameters for our model and this process is called Hyperparameter tuning with model selection . If you are confused here, don't worry we'll see a code example in furhter pages .","title":"Step 7\ufe0f\u20e3"},{"location":"workflow/#step-8","text":"Evaluation \ud83d\udcd2 - After creating model, we are choosing hyperparameter right ??. So how we can confirm that these are right parameters, by checking accuracy. Accuracy is a method in which compare predicted values with real target values. ''More our values are matching with true values, more accurate our model is''. To evaluate, we use metrics (its just like methods to find accuracy), for classification and regression we use different metrics such as |Regression|Classification| |----------|--------------| |Mean Absolute Error|Accuracy| |Mean Squared Error|Precision| |Root Mean Squared Error|F1 score| |Root Mean Logarithmic Error|ROC curve| |Mean Percentage Error|AUC| |Mean Absolute Percentage Error|Log loss| |R square|Average precision| ||Mean Average Precision| ||Confusion Matrix| To know more about evaluation metrics, check sklearn here . After creating the model for high accuracy, don't forget to put it in pipeline","title":"Step 8\ufe0f\u20e3"},{"location":"workflow/#step-9","text":"Test on real world - After model building, test your model on testing set(unknown data). Feed your data to pipeline and pipeline do all the feature engineering and model testing by its own. Key points to remember \ud83d\udd8a\ufe0f Machine learning is an iterative process, more we see data, more our model will learn and more accurate it will become.","title":"Step 9\ufe0f\u20e3"}]}